{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0499d4-669c-48f5-affc-7206d7d520e2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For getting proper stacktrace for exceptions, while using gpu\n",
    "\n",
    "# from helper import cuda_blocking\n",
    "# cuda_blocking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arbitrary-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Libraries\n",
    "\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colonial-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3rd party libraries\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tdata\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "traditional-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "## My custom classes and functions from .py files\n",
    "\n",
    "from path import Path\n",
    "from utils import TokenizedDataset, train_epoch, dev_epoch, save_model, load_model\n",
    "from helper import split_df, iter_batch, csv_data\n",
    "\n",
    "from model import Transformer\n",
    "from lang import load_lang_and_data, compute_lang_and_data, save_lang_and_data, compute_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8426a5-b82f-4e38-b615-bd4e552ff967",
   "metadata": {},
   "source": [
    "## Path object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb252a64-73a6-4837-a646-988add9d7376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: Path \"./AssignmentNLP/output3\" already exist\n",
      "I: Path \"./AssignmentNLP/phase4\" already exist\n",
      "I: Path \"./AssignmentNLP/phase4/answer\" already exist\n",
      "I: Path \"./AssignmentNLP/output3/model\" already exist\n"
     ]
    }
   ],
   "source": [
    "## An object that handles the file locations of generated output files\n",
    "\n",
    "paths = Path('./AssignmentNLP', phase=4, out_ver=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0232ae-eb2e-4a39-b111-d3454ff789df",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unexpected-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse and split the 'train.csv' file into train set and dev set\n",
    "\n",
    "df = csv_data(paths('train.csv'), cols=['hindi', 'english'])\n",
    "train, dev = split_df(df, split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53ef5b2-514c-4047-b10c-5fe4fa45457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default Spacy pipeline has many components that delays the parsing\n",
    "## Hence, use only the necessary components\n",
    "\n",
    "en_sentencizer = English()                    # just the language with no pipeline\n",
    "en_sentencizer.add_pipe('sentencizer')\n",
    "en_lemmatizer = spacy.load('en_core_web_sm', disable=['tok2vec', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a473f85-b831-43c9-a809-ba30dd0fc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute, Save and Load functions for computing, saving and loading\n",
    "##    - Lang object,\n",
    "##    - splitted train and dev set,\n",
    "##    - parsed tokens of train and dev set\n",
    "\n",
    "\n",
    "# (train, dev), (hi_tkns, en_tkns), (dev_hi_tkns, dev_en_tkns), (hi_lang, en_lang) = compute_lang_and_data(train, dev, en_sentencizer, en_lemmatizer)\n",
    "# save_lang_and_data(train, dev, hi_tkns, en_tkns, dev_hi_tkns, dev_en_tkns, hi_lang, en_lang, paths)\n",
    "(train, dev), (hi_tkns, en_tkns), (dev_hi_tkns, dev_en_tkns), (hi_lang, en_lang) = load_lang_and_data(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2674c2f-ce11-4da5-8147-7ddfdefc57a7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56090e6f-6b77-4bd8-a34c-20e14a05b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Few hyperparameters\n",
    "\n",
    "# misc.\n",
    "model_load, model_save, data_parallel = False, True, True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs    = 20\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nhead          = 8\n",
    "embedding_dim  = 512\n",
    "src_pad_idx    = hi_lang[\"[PAD]\"]\n",
    "\n",
    "src_vocab_size     = len(hi_lang)\n",
    "tgt_vocab_size     = len(en_lang)\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "max_src_seq_len = hi_tkns.shape[1]\n",
    "max_tgt_seq_len = en_tkns.shape[1]\n",
    "dim_feedforward = 2048\n",
    "dropout_p       = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dried-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DataLoader\n",
    "\n",
    "# Truncate dev set, so that it won't exceed max sequence length of train set\n",
    "d_hi_tkns, d_en_tkns = dev_hi_tkns[:, :max_src_seq_len], dev_en_tkns[:, :max_tgt_seq_len]\n",
    "\n",
    "# DataLoader\n",
    "train_loader = tdata.DataLoader(dataset=TokenizedDataset(  hi_tkns,   en_tkns), batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader   = tdata.DataLoader(dataset=TokenizedDataset(d_hi_tkns, d_en_tkns), batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emotional-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incase the model parameters are holding GPU memory, release it.\n",
    "\n",
    "if 'scheduler' in globals():\n",
    "    del scheduler\n",
    "    print('I: del scheduler')\n",
    "    if 'optimizer' in globals():\n",
    "        del optimizer\n",
    "        print('I: del optimizer')\n",
    "        if 'model' in globals():\n",
    "            del model\n",
    "            print('I: del model')\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acceptable-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer model, objective and optimizer\n",
    "\n",
    "model = Transformer(embedding_dim, src_pad_idx, nhead,\n",
    "                src_vocab_size, tgt_vocab_size,\n",
    "                num_encoder_layers, num_decoder_layers,\n",
    "                max_src_seq_len, max_tgt_seq_len,\n",
    "                dim_feedforward, dropout_p)\n",
    "\n",
    "if model_load:\n",
    "    model = load_model(model, paths('model.bkp', 10), data_parallel)\n",
    "elif data_parallel:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-notice",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model, print Epoch Loss for both Train and Dev set, finally save the model parameters\n",
    "\n",
    "for epoch_id in range(num_epochs):\n",
    "    train_mean_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device, paths, minibatch=8)\n",
    "    with torch.no_grad():\n",
    "        dev_mean_loss = dev_epoch(model, dev_loader, criterion, device, minibatch=0)\n",
    "    print(f'I: Epoch - {epoch_id}\\t\\tTrainMeanLoss - {train_mean_loss:.3f}\\t\\tDevMeanLoss - {dev_mean_loss:.3f}')\n",
    "    if model_save:\n",
    "        save_model(model, paths('model.bkp', epoch_id), data_parallel)\n",
    "\n",
    "if model_save:\n",
    "    save_model(model, paths('model'), data_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-necklace",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse test phase csv, and compute tokens\n",
    "\n",
    "test      = csv_data(paths('test.csv'), cols=['hindi'])\n",
    "test_tkns = compute_test(test, hi_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy strategy for next token prediction\n",
    "\n",
    "def greedy_prediction(model, src, seq_len):\n",
    "    model.eval()\n",
    "    src_len = src.shape[0]\n",
    "    predictions = [None for _ in range(src_len)]\n",
    "    src = torch.tensor(src, dtype=torch.int64, device=device)\n",
    "    tgt = torch.full((src_len, 1), en_lang['[SOS]'], dtype=torch.int64, device=device)\n",
    "    idx = torch.arange(src_len, dtype=torch.int64)\n",
    "    for i in range(seq_len):\n",
    "        print(f'{i:03d}/{seq_len}', end='\\r')\n",
    "        out = model(src, tgt)\n",
    "        best_guesses = out.argmax(2)[:, -1]\n",
    "        del out\n",
    "        \n",
    "        tgt  = torch.cat((tgt, best_guesses.unsqueeze(1)), dim=1)\n",
    "        mask = torch.ones(best_guesses.shape[0], dtype=torch.bool)\n",
    "        for i,(lang_i,_idx) in enumerate(zip(best_guesses, idx)):\n",
    "            if lang_i == en_lang['[EOS]']:\n",
    "                mask[i] = False\n",
    "                predictions[_idx] = tgt[i, :].squeeze().tolist()\n",
    "        src = src[mask, :]\n",
    "        tgt = tgt[mask, :]\n",
    "        idx = idx[mask]\n",
    "\n",
    "        if not mask.any():\n",
    "            break\n",
    "    for i,tkns in zip(idx, tgt):\n",
    "        predictions[i] = tkns.tolist()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that uses 'greedy_prediction' function and untokenizes the predicted tokens\n",
    "\n",
    "def pred(model, tkns, seq_len):\n",
    "    result = []\n",
    "    pred_all = greedy_prediction(model, tkns, seq_len)\n",
    "    for p in pred_all:\n",
    "        assert p[0] == en_lang['[SOS]']\n",
    "        arr = [en_lang[p[1]].capitalize()]\n",
    "        for idx in p[2:]:\n",
    "            if idx == en_lang['[EOS]']:\n",
    "                break\n",
    "            arr.append(en_lang[idx])\n",
    "        result.append(' '.join(arr))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Piece of code to translate all the hindi sentences in test phase csv, in batch mode\n",
    "\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for s,e in iter_batch(len(test_tkns), 100):\n",
    "        p = pred(model, test_tkns[s:e], max_tgt_seq_len)\n",
    "        result.extend(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-intersection",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Finally, save the translations as output\n",
    "\n",
    "with open(paths('answer.txt', 0), 'w') as outputfile:\n",
    "    outputfile.writelines('\\n'.join(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
